{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Introducing the bikeshare dataset\n",
    "    - Reading in the data\n",
    "    - Visualizing the data\n",
    "2. Linear regression basics\n",
    "    - Form of linear regression\n",
    "    - Building a linear regression model\n",
    "    - Using the model for prediction\n",
    "    - Does the scale of the features matter?\n",
    "3. Working with multiple features\n",
    "    - Visualizing the data (part 2)\n",
    "    - Adding more features to the model\n",
    "4. Choosing between models\n",
    "    - Feature selection\n",
    "    - Evaluation metrics for regression problems\n",
    "    - Comparing models with train/test split and RMSE\n",
    "    - Comparing testing RMSE with null RMSE\n",
    "5. Creating features\n",
    "    - Handling categorical features\n",
    "    - Feature engineering\n",
    "6. Comparing linear regression with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data\n",
    "\n",
    "We'll be working with a dataset from Capital Bikeshare that was used in a Kaggle competition ([data dictionary](https://www.kaggle.com/c/bike-sharing-demand/data))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data and set the datetime as the index\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "import pandas as pd\n",
    "urls = ['../data/KDCA-201601.csv', '../data/KDCA-201602.csv', '../data/KDCA-201603.csv']\n",
    "frames = [pd.read_csv(url) for url in urls]\n",
    "weather = pd.concat(frames)\n",
    "cols = 'WBAN\tDate\tTime\tStationType\tSkyCondition\tVisibility\tWeatherType\tDryBulbFarenheit\tDryBulbCelsius\tWetBulbFarenheit\tWetBulbCelsius\tDewPointFarenheit\tDewPointCelsius\tRelativeHumidity\tWindSpeed\tWindDirection\tValueForWindCharacter\tStationPressure\tPressureTendency\tPressureChange\tSeaLevelPressure\tRecordType\tHourlyPrecip\tAltimeter'\n",
    "cols = cols.split()\n",
    "weather = weather[cols]\n",
    "weather.rename(columns={'DryBulbFarenheit':'temp',\n",
    "                       'RelativeHumidity': 'humidity'}, inplace=True)\n",
    "# weather['humidity'] = pd.to_numeric(weather.humidity, errors='coerce')\n",
    "\n",
    "weather['datetime'] = pd.to_datetime(weather.Date.astype(str) + weather.Time.apply('{0:0>4}'.format))\n",
    "weather['datetime_hour'] = weather.datetime.dt.floor(freq='h')\n",
    "weather['month'] = weather.datetime.dt.month\n",
    "\n",
    "bikes = pd.read_csv('../data/2016-Q1-Trips-History-Data.csv')\n",
    "bikes['start'] = pd.to_datetime(bikes['Start date'], infer_datetime_format=True)\n",
    "bikes['end'] = pd.to_datetime(bikes['End date'], infer_datetime_format=True)\n",
    "bikes['datetime_hour'] = bikes.start.dt.floor(freq='h')\n",
    "weather[['datetime', 'temp']].hist(bins=30)\n",
    "print(weather.columns)\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.merge(weather[['temp', 'datetime_hour', 'datetime']], on='datetime_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = bikes.groupby('datetime_hour').agg('count')\n",
    "hours['datetime_hour'] = hours.index\n",
    "hours.head()\n",
    "hours['total'] = hours.start\n",
    "hours = hours[['total', 'datetime_hour']]\n",
    "hours.total.plot()\n",
    "hours_weather = hours.merge(weather, on='datetime_hour')\n",
    "hours_weather.plot(kind='scatter', x='temp', y='total')\n",
    "sns.lmplot(x='temp', y='total', data=hours_weather, aspect=1.5, scatter_kws={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = hours_weather[(hours_weather.datetime.dt.hour==11) & (hours_weather.datetime.dt.dayofweek<5) ]\n",
    "weekday.plot(kind='scatter', x='temp', y='total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "sns.lmplot(x='temp', y='total', data=weekday, aspect=1.5, scatter_kws={'alpha':0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "- What does each observation represent?\n",
    "- What is the response variable (as defined by Kaggle)?\n",
    "- How many features are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form of linear regression\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n$\n",
    "\n",
    "- $y$ is the response\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature)\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature)\n",
    "\n",
    "The $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- And once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](../data/estimating_coefficients.png)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['temp']\n",
    "X = hours_weather[feature_cols]\n",
    "y = hours_weather.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the coefficients\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the **intercept** ($\\beta_0$):\n",
    "\n",
    "- It is the value of $y$ when $x$=0.\n",
    "- Thus, it is the estimated number of rentals when the temperature is 0 degrees Celsius.\n",
    "- **Note:** It does not always make sense to interpret the intercept. (Why?)\n",
    "\n",
    "Interpreting the **\"temp\" coefficient** ($\\beta_1$):\n",
    "\n",
    "- It is the change in $y$ divided by change in $x$, or the \"slope\".\n",
    "- Thus, a temperature increase of 1 degree F is **associated with** a rental increase of 9.17 bikes.\n",
    "- This is not a statement of causation.\n",
    "- $\\beta_1$ would be **negative** if an increase in temperature was associated with a **decrease** in rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model for prediction\n",
    "\n",
    "How many bike rentals would we predict if the temperature was 77 degrees F?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually calculate the prediction\n",
    "linreg.intercept_ + linreg.coef_ * 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the predict method\n",
    "linreg.predict(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does the scale of the features matter?\n",
    "\n",
    "Let's say that temperature was measured in Fahrenheit, rather than Celsius. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for Fahrenheit temperature\n",
    "hours_weather['temp_C'] = (hours_weather.temp - 32) * 5/9\n",
    "hours_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatter plot with regression line\n",
    "sns.lmplot(x='temp_C', y='total', data=hours_weather, aspect=1.5, scatter_kws={'alpha':0.2})\n",
    "sns.lmplot(x='temp', y='total', data=hours_weather, aspect=1.5, scatter_kws={'alpha':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "feature_cols = ['temp_C']\n",
    "X = hours_weather[feature_cols]\n",
    "y = hours_weather.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(linreg.intercept_, linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 77 degrees Fahrenheit to Celsius\n",
    "(77 - 32)* 5/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict rentals for 25 degrees Celsius\n",
    "linreg.predict([[25], [30]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The scale of the features is **irrelevant** for linear regression models. When changing the scale, we simply change our **interpretation** of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove the temp_F column\n",
    "# bikes.drop('temp_C', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data (part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore more features\n",
    "feature_cols = ['temp', 'month', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatter plots in Seaborn\n",
    "# print(hours_weather.humidity != 'M')\n",
    "hours_weather.humidity = hours_weather.humidity.apply(lambda x: -1 if isinstance(x, str) else x)\n",
    "# hours_weather.loc[hours_weather.humidity.dtype != int].humidity = 100\n",
    "sns.pairplot(hours_weather, x_vars=feature_cols, y_vars='total', kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatter plots in Pandas\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    hours_weather.plot(kind='scatter', x=feature, y='total', ax=axs[index], figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you seeing anything that you did not expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-tabulation of season and month\n",
    "pd.crosstab(hours_weather.month, hours_weather.datetime.dt.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot of rentals, grouped by season\n",
    "hours_weather.boxplot(column='total', by='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot of rentals\n",
    "hours_weather.total.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing **overall growth** and the winter months happen to come after the spring months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix (ranges from 1 to -1)\n",
    "hours_weather.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(hours_weather.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What relationships do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more features to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of features\n",
    "feature_cols = ['temp', 'month', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X and y\n",
    "X = hours_weather[feature_cols]\n",
    "y = hours_weather.total\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print(linreg.intercept_, linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair the feature names with the coefficients\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "- Holding all other features fixed, a 1 unit increase in **temperature** is associated with a **rental increase of 9.3 bikes**.\n",
    "- Holding all other features fixed, a 1 unit increase in **month** is associated with a **rental increase of 30.6 bikes**.\n",
    "- Holding all other features fixed, a 1 unit increase in **humidity** is associated with a **rental decrease of .60 bikes**.\n",
    "\n",
    "Does anything look incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "How do we choose which features to include in the model? We're going to use **train/test split** (and eventually **cross-validation**).\n",
    "\n",
    "Why not use of **p-values** or **R-squared** for feature selection?\n",
    "\n",
    "- Linear models rely upon **a lot of assumptions** (such as the features being independent), and if those assumptions are violated, p-values and R-squared are less reliable. Train/test split relies on fewer assumptions.\n",
    "- Features that are unrelated to the response can still have **significant p-values**.\n",
    "- Adding features to your model that are unrelated to the response will always **increase the R-squared value**, and adjusted R-squared does not sufficiently account for this.\n",
    "- p-values and R-squared are **proxies** for our goal of generalization, whereas train/test split and cross-validation attempt to **directly estimate** how well the model will generalize to out-of-sample data.\n",
    "\n",
    "More generally:\n",
    "\n",
    "- There are different methodologies that can be used for solving any given data science problem, and this course follows a **machine learning methodology**.\n",
    "- This course focuses on **general purpose approaches** that can be applied to any model, rather than model-specific approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics for regression problems\n",
    "\n",
    "Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example true and predicted response values\n",
    "true = [10, 7, 5, 5]\n",
    "pred = [8, 6, 5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these metrics:\n",
    "\n",
    "- **MAE** is the easiest to understand, because it's the average error.\n",
    "- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "Here's an additional example, to demonstrate how MSE/RMSE punish larger errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same true values as above\n",
    "true = [10, 7, 5, 5]\n",
    "\n",
    "# new set of predicted values\n",
    "pred = [10, 7, 5, 13]\n",
    "\n",
    "# MAE is the same as before\n",
    "print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    "\n",
    "# MSE and RMSE are larger than before\n",
    "print('MSE:', metrics.mean_squared_error(true, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))\n",
    "rmse = np.sqrt(metrics.mean_squared_error(true, pred))\n",
    "rmse/pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models with train/test split and RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "\n",
    "# define a function that accepts a list of features and returns testing RMSE\n",
    "def train_test_rmse(feature_cols, data):\n",
    "    X = data[feature_cols]\n",
    "    y = data.total\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare different sets of features\n",
    "print(train_test_rmse(['temp', 'month', 'humidity'], hours_weather))\n",
    "print(train_test_rmse(['temp', 'month'], hours_weather))\n",
    "print(train_test_rmse(['temp', 'humidity'], hours_weather))\n",
    "print(train_test_rmse(['temp'], hours_weather))\n",
    "print(train_test_rmse(['temp'], weekday))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing testing RMSE with null RMSE\n",
    "\n",
    "Null RMSE is the RMSE that could be achieved by **always predicting the mean response value**. It is a benchmark against which you may want to measure your regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(weekday[['temp']], weekday.total, random_state=123)\n",
    "\n",
    "# create a NumPy array with the same shape as y_test\n",
    "y_null = np.zeros_like(y_test, dtype=float)\n",
    "\n",
    "# fill the array with the mean value of y_test\n",
    "y_null.fill(y_test.mean())\n",
    "y_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute null RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "- **Ordered categories:** transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "- **Unordered categories:** use dummy encoding (0/1)\n",
    "\n",
    "What are the categorical features in our dataset?\n",
    "\n",
    "- **Ordered categories:** weather (already encoded with sensible numeric values)\n",
    "- **Unordered categories:** season (needs dummy encoding), holiday (already dummy encoded), workingday (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an **ordered relationship**. Instead, we create **multiple dummy variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "season_dummies = pd.get_dummies(hours_weather.month, prefix='month')\n",
    "\n",
    "# print 5 random rows\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have a categorical feature with **k possible values**, you create **k-1 dummy variables**.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for holiday, not two dummy variables (holiday_yes and holiday_no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "hw_dum = pd.concat([hours_weather, season_dummies], axis=1)\n",
    "\n",
    "# print 5 random rows\n",
    "hw_dum.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include dummy variables for season in the model\n",
    "feature_cols = ['temp','month_1', 'month_2', 'month_3', 'humidity']\n",
    "X = hw_dum[feature_cols]\n",
    "y = hw_dum.total\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare original season variable with dummy variables\n",
    "print(train_test_rmse(['temp', 'month', 'humidity'], hw_dum))\n",
    "print(train_test_rmse(['temp', 'month_2', 'month', 'humidity'], hw_dum))\n",
    "print(train_test_rmse(['temp', 'month_2', 'month_1', 'humidity'], hw_dum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "See if you can create the following features:\n",
    "\n",
    "- **hour:** as a single numeric feature (0 through 23)\n",
    "- **hour:** as a categorical feature (use 23 dummy variables)\n",
    "- **daytime:** as a single categorical feature (daytime=1 from 7am to 8pm, and daytime=0 otherwise)\n",
    "\n",
    "Then, try using each of the three features (on its own) with `train_test_rmse` to see which one performs the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hour as a numeric feature\n",
    "hw_dum['hour'] = hw_dum.datetime.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hour as a categorical feature\n",
    "hour_dummies = pd.get_dummies(hw_dum.hour, prefix='hour')\n",
    "# hour_dummies.drop(hour_dummies.columns[0], axis=1, inplace=True)\n",
    "hw_dum = pd.concat([hw_dum, hour_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# daytime as a categorical feature\n",
    "hw_dum['daytime'] = ((hw_dum.hour > 6) & (hw_dum.hour < 21)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_test_rmse(['hour'], hw_dum),\n",
    "    train_test_rmse(hw_dum.columns[hw_dum.columns.str.startswith('hour_')], hw_dum)\n",
    "    ,train_test_rmse(['daytime'], hw_dum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing linear regression with other models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain\n",
    "- Highly interpretable\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Well-understood\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "- Can't automatically learn feature interactions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
